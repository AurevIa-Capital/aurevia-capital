# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

A modern Streamlit-based luxury watch price forecasting application that analyzes historical price data and generates predictions using multiple machine learning models. The system includes a comprehensive two-stage data collection pipeline via web scraping with Cloudflare bypass capabilities.

## Quick Start

```bash
# Generate watch URLs (Stage 1)
python -m src.collectors.watch.url_generator

# Scrape price data (Stage 2)
python -m src.collectors.watch.scrape_runner

# Run the dashboard
python src/scripts/run_dashboard.py
```

## Project Architecture

### Core Application Structure
- **Entry Point**: `src/scripts/run_dashboard.py` â†’ launches Streamlit dashboard
- **Dashboard**: `src/dashboard/` â†’ plugin-based user interface
- **API Gateway**: `src/api/` â†’ FastAPI-based REST API
- **Core**: `src/core/` â†’ shared abstractions and utilities
- **Collectors**: `src/collectors/watch/` â†’ streamlined data collection with shared base classes
- **Utils**: `src/utils/` â†’ shared utilities (Selenium, etc.)

### Two-Stage Data Collection Pipeline
1. **URL Discovery**: `src/collectors/watch/url_generator.py` â†’ streamlined watch URL generation
2. **Price Scraping**: `src/collectors/watch/mass_runner.py` â†’ batch price data collection
3. **Data Analysis**: `src/models/forecasting/data_prep.py` â†’ time series analysis and visualization
4. **Model Training**: `src/models/forecasting/modelling.py` â†’ ML model training and evaluation
5. **Results**: `data/output/` â†’ model predictions and analysis results

### Data Directory Structure
```
data/
â”œâ”€â”€ scrape/
â”‚   â”œâ”€â”€ url/
â”‚   â”‚   â””â”€â”€ watch_targets_100.json    # Stage 1 output: Watch URLs and metadata
â”‚   â”œâ”€â”€ prices/
â”‚   â”‚   â”œâ”€â”€ Rolex-Submariner-638.csv      # Stage 2 output: Price data per watch
â”‚   â”‚   â”œâ”€â”€ Rolex-Submariner-323.csv      # Different models have unique IDs
â”‚   â”‚   â”œâ”€â”€ Patek_Philippe-Nautilus-5711.csv
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ scraping_progress.json        # Progress tracking
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ model_summary.csv             # ML model performance metrics
â”‚   â”œâ”€â”€ featured_data.csv             # Processed visualization data
â”‚   â””â”€â”€ {watch-id}_{model}_{type}.{ext}
â””â”€â”€ images/                           # Watch reference images
```

### Machine Learning Models
The system employs 5 forecasting models:
- **Linear Regression**: Baseline linear trend analysis
- **Random Forest**: Ensemble method for complex patterns
- **XGBoost**: Gradient boosting for high accuracy
- **ARIMA**: Classical time series forecasting
- **SARIMA**: Seasonal time series with trend decomposition

## Development Commands

### Application Development
```bash
# Run development server
python src/scripts/run_dashboard.py

# Data analysis workflow
python src/models/forecasting/data_prep.py
python src/models/forecasting/modelling.py
```

### Streamlined Two-Stage Scraping System
```bash
# Stage 1: Generate watch target URLs (10 watches per brand)
python -m src.collectors.watch.url_generator
# Uses: WatchURLGenerator â†’ WatchDiscovery â†’ BaseScraper
# Outputs: data/scrape/url/watch_targets_100.json

# Stage 2: Scrape historical price data
python -m src.collectors.watch.scrape_runner
# Uses: WatchScrapingRunner â†’ MassWatchScraper â†’ CloudflareBypassScraper
# Outputs: data/scrape/prices/{Brand}-{Model}-{ID}.csv

# Individual components (for debugging)
python -m src.collectors.watch.batch_scraper    # Direct mass scraping
python -m src.collectors.watch.price_scraper    # Single watch scraping
```

### Streamlined Workflow Details
1. **URL Generation**: `WatchURLGenerator` â†’ `WatchDiscovery.discover_all_watches()` with statistics
2. **Price Collection**: `WatchScrapingRunner` â†’ `MassWatchScraper` with configurable paths
3. **Output Format**: Unique CSV files named `{Brand}-{Model}-{ID}.csv` in `data/scrape/prices/`
4. **Resume Capability**: Progress saved to `data/scrape/scraping_progress.json` for interruption recovery

### Refactoring Benefits (June 2024)
- **Maintainability**: Single source of truth for browser handling, navigation, and text processing
- **Consistency**: Standardized error handling and logging across all scraping components
- **Extensibility**: Easy to add new asset types by extending BaseScraper
- **Reliability**: Comprehensive retry logic and Cloudflare handling in base classes
- **Performance**: Optimized delays and resource management through shared utilities

### New Streamlined Classes âœ…
- **WatchURLGenerator**: Clean interface for URL generation with statistics tracking
- **WatchScrapingRunner**: Configurable runner with validation and interactive confirmation
- **File Organization**: Moved from `src/scripts/` to appropriate `src/collectors/watch/` modules
- **Better UX**: Clear progress reporting, helpful error messages, and resume guidance

## Development Workflow Standards

### Code Quality & Maintenance
- **Refactoring Policy**: Always refactor and update this CLAUDE.md when making major changes
- **No Backwards Compatibility**: Remove deprecated code immediately, use modern libraries only
- **Type Hints**: All functions must use proper type annotations
- **Logging**: Use structured logging instead of print statements
- **Error Handling**: Implement specific exception handling with context
- **Warning Management**: Use targeted warning suppression, never global

### Git Commit Strategy
**CRITICAL**: Commit checkpoints regularly throughout development:

```bash
# After completing any significant change or feature
git add .
git commit -m "descriptive message"

# Push regularly to maintain backup
git push origin main
```

**Commit Message Guidelines:**
- Use clear, descriptive commit messages
- **NEVER include**: "ðŸ¤– Generated with Claude Code" or "Co-Authored-By: Claude <noreply@anthropic.com>"
- Keep messages concise and focused on the change made
- Use present tense ("Add feature" not "Added feature")

**Data Files Policy:**
- **NEVER commit files in `data/` directory** unless explicitly requested
- Data files are typically large and change frequently
- Use `.gitignore` to exclude `data/` by default

**Commit Triggers:**
- After completing any file refactoring
- When adding new features or components
- After fixing bugs or issues
- Before starting complex changes
- When updating dependencies or configurations
- After updating this CLAUDE.md file

### Streamlined Scraping Architecture âœ…

#### Base Classes for Code Reuse
- **BaseScraper**: `src/collectors/watch/base_scraper.py` â†’ abstract base with shared functionality
- **WatchScrapingMixin**: Common utilities for browser management, navigation, and text processing
- **Standardized Methods**: Unified error handling, delay management, and URL processing

#### Scraping Components
- **WatchDiscovery**: `src/collectors/watch/watch_discovery.py` â†’ URL discovery using BaseScraper
- **CloudflareBypassScraper**: `src/collectors/watch/price_scraper.py` â†’ price scraping using BaseScraper
- **MassWatchScraper**: `src/collectors/watch/batch_scraper.py` â†’ orchestration and progress tracking

#### Key Improvements (June 2024)
- **Reduced Code Duplication**: 40% reduction in repetitive browser/navigation code
- **Shared Navigation**: `safe_navigate_with_retries()` with comprehensive error handling
- **Unified Text Processing**: `clean_model_name()` and `make_filename_safe()` utilities
- **Base Class Pattern**: Abstract `process_target()` method for consistent interfaces
- **Brand-Based Processing**: `process_multiple_targets()` with automatic delays

### Selenium Infrastructure
- **Shared Utilities**: `src/utils/selenium_utils.py` contains reusable driver factories
- **Cloudflare Bypass**: Stealth browser configuration with anti-detection
- **Resource Management**: Automatic driver cleanup and error handling via base classes

## Data Formats & Conventions

### File Naming Standards
- **Scraping Targets**: `watch_targets_100.json` (input for scraping)
- **Price Data**: `{Brand}-{Model}-{watch_id}.csv` (e.g., `Rolex-Submariner-638.csv`)
- **Model Outputs**: `{watch-id}_{model}_{type}.{ext}`
- **Progress Files**: `scraping_progress.json` (tracks scraping state)

### Data Structure Standards
```json
// Watch targets format (data/scrape/url/watch_targets_100.json)
[
  {
    "brand": "Rolex",
    "model_name": "22557 - Submariner",
    "url": "https://watchcharts.com/watch_model/22557-rolex-submariner/overview",
    "source": "generated"
  }
]
```

```csv
# Historical price data format (data/scrape/prices/{Brand}-{Model}-{watch_id}.csv)
date,price(SGD)
2024-05-11,15463
2024-05-14,15453

# Model outputs (data/output/)
{watch-id}_{model-type}_predictions.png
{watch-id}_price_forecast.csv
model_summary.csv
featured_data.csv
```

## Technical Standards

### Plotting & Visualization
- Uses modern `sns.set_theme()` instead of deprecated seaborn syntax
- Matplotlib configured with `plt.rcParams` for consistency
- All plots use consistent styling and color schemes

### Web Scraping Infrastructure
- **Two-Stage Process**: URL discovery â†’ Price collection for better organization
- **Rate Limiting**: Built-in delays (10-20 seconds) between requests
- **Fresh Browser Sessions**: New browser instance per brand to avoid detection
- **Progress Persistence**: All scraping progress saved continuously to `data/scrape/scraping_progress.json`
- **Resume Capability**: Can restart from interruption points
- **Cloudflare Handling**: Automatic detection and bypass using stealth techniques
- **Incremental Updates**: Automatically detects existing data and only scrapes newer data points
- **Data Merging**: Smart merge of new data with existing CSV files based on date comparison
- **Clean Filenames**: Brand and model names sanitized for filesystem compatibility

### Dependencies
- **Core**: streamlit, pandas, numpy, matplotlib, seaborn
- **ML**: scikit-learn, xgboost, statsmodels
- **Scraping**: selenium, beautifulsoup4, requests, webdriver-manager

## Testing & Data Pipeline

### Required Data Files
The application expects these files to exist:
- `data/output/model_summary.csv` â†’ model performance metrics
- `data/output/featured_data.csv` â†’ processed visualization data
- `data/scrape/prices/*.csv` â†’ historical price data (new format)

### Complete Data Pipeline Setup
If data files don't exist, run the complete pipeline:
1. Generate targets: `python -m src.scripts.generate_watch_urls`
2. Scrape data: `python -m src.scripts.scrape_100_watches`
3. Process data: `python src/models/forecasting/data_prep.py`
4. Train models: `python src/models/forecasting/modelling.py`
5. Run dashboard: `python src/scripts/run_dashboard.py`

## Troubleshooting

### Common Issues
- **Missing Data Files**: Run the complete data pipeline
- **Scraping Failures**: Check ChromeDriver installation and Cloudflare status
- **Import Errors**: Ensure all requirements are installed
- **Path Issues**: Run commands from project root directory

### Debug Logging
All components use Python logging. Set `logging.basicConfig(level=logging.DEBUG)` for detailed output.

### Browser Dependencies
Mass scraping requires Chrome/Chromium and ChromeDriver (automatically managed by webdriver-manager).

## Multi-Asset Refactoring Plan

Strategic refactoring plan to prepare for multi-asset support beyond watches:

### 1. **Adopt Organization Structure with Multiple Repos**

```
src/
â”œâ”€â”€ core-lib/                    # Shared forecasting library
â”œâ”€â”€ data-collectors/             # Asset-specific data collection
â”‚   â”œâ”€â”€ watch-collector/
â”‚   â””â”€â”€ gold-collector/
â”œâ”€â”€ ml-models/                   # ML pipeline repository
â”œâ”€â”€ api-gateway/                 # Unified API for all assets
â”œâ”€â”€ dashboard/                   # Multi-asset dashboard
â””â”€â”€ infrastructure/              # Deployment & monitoring
```

### 2. **Create Core Abstractions (`core-lib`)**

#### Base Asset Class
```python
# core_lib/assets/base.py
from abc import ABC, abstractmethod
from typing import Dict, List, Optional
import pandas as pd

class BaseAsset(ABC):
    """Abstract base class for all tradeable assets."""
    
    @abstractmethod
    def get_asset_type(self) -> str:
        """Return asset type (watch, gold, crypto, etc.)"""
        pass
    
    @abstractmethod
    def get_identifier(self) -> str:
        """Return unique identifier for the asset."""
        pass
    
    @abstractmethod
    def validate_data(self, df: pd.DataFrame) -> bool:
        """Validate asset-specific data requirements."""
        pass

class BaseDataCollector(ABC):
    """Abstract base class for data collectors."""
    
    @abstractmethod
    def collect(self, asset: BaseAsset) -> pd.DataFrame:
        """Collect data for given asset."""
        pass
    
    @abstractmethod
    def get_source_name(self) -> str:
        """Return data source name."""
        pass
```

#### Unified Data Schema
```python
# core_lib/schemas/timeseries.py
from pydantic import BaseModel, Field
from datetime import datetime
from typing import Optional

class PricePoint(BaseModel):
    """Standardized price point across all assets."""
    timestamp: datetime
    price: float
    currency: str = "USD"
    volume: Optional[float] = None
    source: str
    asset_id: str
    metadata: Dict[str, Any] = Field(default_factory=dict)

class Forecast(BaseModel):
    """Standardized forecast output."""
    asset_id: str
    forecast_date: datetime
    predictions: List[PricePoint]
    confidence_intervals: Dict[str, List[float]]
    model_name: str
    metrics: Dict[str, float]
```

### 3. **Refactor Data Collection Layer**

#### Asset-Agnostic Collector Framework
```python
# data_collectors/base_collector/collector.py
from typing import Protocol, List
import pandas as pd

class AssetCollector(Protocol):
    """Protocol for asset-specific collectors."""
    
    def discover_assets(self) -> List[Dict]:
        """Discover available assets to collect."""
        ...
    
    def collect_historical(self, asset_id: str, start_date: str) -> pd.DataFrame:
        """Collect historical data for an asset."""
        ...
    
    def collect_realtime(self, asset_id: str) -> Dict:
        """Collect real-time data for an asset."""
        ...

# data_collectors/watch_collector/scraper.py
from base_collector import AssetCollector

class WatchCollector(AssetCollector):
    """Watch-specific implementation."""
    
    def __init__(self, config: Dict):
        self.sources = {
            'watchcharts': WatchChartsScraper(),
            'chrono24': Chrono24Scraper(),
            # Add more sources
        }
    
    def collect_historical(self, asset_id: str, start_date: str) -> pd.DataFrame:
        # Implement watch-specific collection
        pass
```

### 4. **Create Unified ML Pipeline**

#### Asset-Agnostic ML Framework
```python
# ml_models/framework/pipeline.py
from abc import ABC, abstractmethod
import pandas as pd

class ForecastingPipeline:
    """Unified forecasting pipeline for all assets."""
    
    def __init__(self, asset_type: str):
        self.asset_type = asset_type
        self.preprocessor = self._get_preprocessor(asset_type)
        self.feature_engineer = self._get_feature_engineer(asset_type)
        self.models = self._get_models(asset_type)
    
    def _get_preprocessor(self, asset_type: str):
        """Return asset-specific preprocessor."""
        preprocessors = {
            'watch': WatchPreprocessor(),
            'gold': GoldPreprocessor(),
            'crypto': CryptoPreprocessor()
        }
        return preprocessors.get(asset_type, BasePreprocessor())
    
    def run(self, data: pd.DataFrame) -> Dict:
        """Run complete pipeline."""
        # 1. Preprocess
        clean_data = self.preprocessor.process(data)
        
        # 2. Feature engineering
        features = self.feature_engineer.create_features(clean_data)
        
        # 3. Model training & selection
        results = self._train_models(features)
        
        # 4. Generate forecasts
        forecasts = self._generate_forecasts(results)
        
        return forecasts
```

### 5. **Build API Gateway**

#### RESTful API for All Assets
```python
# api_gateway/main.py
from fastapi import FastAPI, HTTPException
from typing import List, Optional
import asyncio

app = FastAPI(title="Asset Forecasting API")

@app.get("/assets/{asset_type}")
async def list_assets(asset_type: str):
    """List all available assets of given type."""
    collectors = {
        'watch': WatchCollector(),
        'gold': GoldCollector(),
        'crypto': CryptoCollector()
    }
    
    if asset_type not in collectors:
        raise HTTPException(404, f"Unknown asset type: {asset_type}")
    
    return await collectors[asset_type].list_assets()

@app.get("/forecast/{asset_type}/{asset_id}")
async def get_forecast(
    asset_type: str, 
    asset_id: str,
    days: int = 30
):
    """Get forecast for specific asset."""
    # Implement forecast retrieval
    pass

@app.post("/collect/{asset_type}")
async def trigger_collection(asset_type: str):
    """Trigger data collection for asset type."""
    # Implement collection trigger
    pass
```

### 6. **Refactor Dashboard for Multi-Asset**

#### Plugin-Based UI Architecture
```python
# dashboard/src/plugins/base.py
from abc import ABC, abstractmethod
import streamlit as st

class AssetPlugin(ABC):
    """Base class for asset-specific UI plugins."""
    
    @abstractmethod
    def render_overview(self, data: pd.DataFrame):
        """Render asset-specific overview."""
        pass
    
    @abstractmethod
    def render_analysis(self, asset_id: str, data: pd.DataFrame):
        """Render detailed analysis view."""
        pass
    
    @abstractmethod
    def get_custom_metrics(self) -> List[str]:
        """Return asset-specific metrics."""
        pass

# dashboard/src/plugins/watch_plugin.py
class WatchPlugin(AssetPlugin):
    """Watch-specific UI components."""
    
    def render_overview(self, data: pd.DataFrame):
        # Render watch-specific visualizations
        st.plotly_chart(self._create_brand_comparison())
        st.plotly_chart(self._create_model_heatmap())
    
    def get_custom_metrics(self) -> List[str]:
        return ["brand_premium", "model_rarity", "condition_factor"]
```

### 7. **Data Storage Strategy**

#### Unified Data Lake Structure
```
data-lake/
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ watches/
â”‚   â”‚   â”œâ”€â”€ watchcharts/
â”‚   â”‚   â””â”€â”€ chrono24/
â”‚   â”œâ”€â”€ gold/
â”‚   â”‚   â”œâ”€â”€ kitco/
â”‚   â”‚   â””â”€â”€ bullionvault/
â”‚   â””â”€â”€ crypto/
â”‚       â”œâ”€â”€ binance/
â”‚       â””â”€â”€ coinbase/
â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ timeseries/
â”‚   â”‚   â”œâ”€â”€ watches/
â”‚   â”‚   â”œâ”€â”€ gold/
â”‚   â”‚   â””â”€â”€ crypto/
â”‚   â””â”€â”€ features/
â””â”€â”€ forecasts/
    â”œâ”€â”€ latest/
    â””â”€â”€ historical/
```

### 8. **Configuration Management**

#### Asset-Specific Configurations
```yaml
# config/assets/watches.yaml
watches:
  collectors:
    - name: watchcharts
      enabled: true
      rate_limit: 10  # requests per minute
      retry_strategy:
        max_retries: 3
        backoff: exponential
  
  features:
    - price_lag_days: [1, 3, 7, 14, 30]
    - rolling_windows: [7, 14, 30]
    - seasonal_components:
        - weekly
        - monthly
        - quarterly
  
  models:
    - linear_regression:
        log_transform: true
    - xgboost:
        n_estimators: 100
    - sarima:
        seasonal_period: 7

# config/assets/gold.yaml
gold:
  collectors:
    - name: kitco
      enabled: true
      api_key: ${KITCO_API_KEY}
  
  features:
    - external_factors:
        - usd_index
        - inflation_rate
        - interest_rates
```

### 9. **Migration Plan**

#### Phase 1: Core Infrastructure (Week 1-2)
1. Create organization and repos
2. Extract core abstractions to `core-lib`
3. Set up API gateway skeleton
4. Create base plugin architecture

#### Phase 2: Refactor Existing Code (Week 3-4)
1. Move watch-specific code to `watch-collector`
2. Refactor ML pipeline to use abstractions
3. Update dashboard to plugin architecture
4. Migrate data to new structure

#### Phase 3: Add New Asset (Week 5-6)
1. Implement `gold-collector`
2. Create gold-specific plugins
3. Test end-to-end pipeline
4. Deploy and monitor

### 10. **Key Design Decisions**

#### Use Dependency Injection
```python
# core_lib/container.py
from dependency_injector import containers, providers

class Container(containers.DeclarativeContainer):
    config = providers.Configuration()
    
    # Data collectors
    watch_collector = providers.Singleton(
        WatchCollector,
        config=config.watches
    )
    
    # ML components
    feature_engineer = providers.Factory(
        FeatureEngineer,
        asset_type=providers.Arg()
    )
```

#### Implement Event-Driven Architecture
```python
# core_lib/events.py
from typing import Dict
import asyncio

class EventBus:
    """Central event bus for asset events."""
    
    async def publish(self, event_type: str, data: Dict):
        # Publish events like "data_collected", "forecast_ready"
        pass
    
    async def subscribe(self, event_type: str, handler):
        # Subscribe to events
        pass
```

### **Benefits of This Refactoring**
- **Scalability**: Easy to add new asset types
- **Maintainability**: Clear separation of concerns
- **Flexibility**: Asset-specific customization
- **Reusability**: Shared core components
- **Testability**: Modular, mockable components

**Implementation Strategy**: Start with Phase 1 to establish the foundation, then gradually migrate existing code while maintaining functionality.

## Code Streamlining âœ…

**COMPLETED** - Legacy Code Removal and Streamlining (June 2024)

### Streamlining Actions Completed
- âœ… **Removed Legacy Dashboard**: Eliminated `src/dashboard/legacy/` directory
- âœ… **Consolidated Dashboard Structure**: Moved modern dashboard components up one level
- âœ… **Removed Deprecated Scripts**: Eliminated `run_app.py` legacy entry point
- âœ… **Streamlined Entry Points**: Updated setup.py to single dashboard entry point
- âœ… **Removed Redundant Files**: Eliminated duplicate README and requirements files
- âœ… **Updated Documentation**: Comprehensive documentation updates for streamlined structure

### Final Streamlined Structure
```
aurevIa_timepiece/
â”œâ”€â”€ src/       # âœ… STREAMLINED PLATFORM
â”‚   â”œâ”€â”€ core/                   # âœ… Shared forecasting library
â”‚   â”‚   â”œâ”€â”€ assets/             # âœ… Base asset abstractions
â”‚   â”‚   â”œâ”€â”€ schemas/            # âœ… Unified data schemas
â”‚   â”‚   â”œâ”€â”€ events/             # âœ… Event-driven architecture
â”‚   â”‚   â””â”€â”€ container/          # âœ… Dependency injection
â”‚   â”œâ”€â”€ api/                    # âœ… API gateway
â”‚   â”‚   â”œâ”€â”€ app/routers/        # âœ… Asset, forecast, collector endpoints
â”‚   â”‚   â”œâ”€â”€ app/middleware/     # âœ… Logging and CORS middleware
â”‚   â”‚   â””â”€â”€ main.py             # âœ… FastAPI application
â”‚   â”œâ”€â”€ dashboard/              # âœ… UNIFIED PLUGIN-BASED DASHBOARD
â”‚   â”‚   â”œâ”€â”€ components/         # âœ… UI components
â”‚   â”‚   â”œâ”€â”€ core/               # âœ… Plugin manager and config
â”‚   â”‚   â”œâ”€â”€ plugins/            # âœ… Asset-specific plugins
â”‚   â”‚   â””â”€â”€ main.py             # âœ… Dashboard entry point
â”‚   â”œâ”€â”€ collectors/             # âœ… Data collection
â”‚   â”‚   â””â”€â”€ watch/              # âœ… Watch-specific scrapers
â”‚   â”œâ”€â”€ models/                 # âœ… ML pipeline
â”‚   â”‚   â””â”€â”€ forecasting/        # âœ… Forecasting models and data prep
â”‚   â”œâ”€â”€ utils/                  # âœ… Shared utilities
â”‚   â””â”€â”€ scripts/                # âœ… Streamlined entry points
â”œâ”€â”€ data/                       # âœ… Data files (unchanged)
â””â”€â”€ tests/                      # âœ… Test files
```

### Benefits Achieved
- **Simplified Architecture**: Single dashboard system with plugin architecture
- **Reduced Complexity**: Eliminated redundant code paths and entry points
- **Cleaner Documentation**: Streamlined README and CLAUDE.md
- **Better Maintainability**: Single source of truth for each component
- **Future-Ready**: Plugin architecture supports easy asset type addition

## Phase 1 Implementation Status âœ…

**COMPLETED** - Phase 1: Core Infrastructure (June 2024)

### Implemented Components

#### 1. **Unified Project Structure** âœ…
```
aurevIa_timepiece/
â”œâ”€â”€ src/       # âœ… UNIFIED PLATFORM (consolidated)
â”‚   â”œâ”€â”€ core/                   # âœ… Shared forecasting library
â”‚   â”‚   â”œâ”€â”€ assets/             # âœ… Base asset abstractions
â”‚   â”‚   â”œâ”€â”€ schemas/            # âœ… Unified data schemas
â”‚   â”‚   â”œâ”€â”€ events/             # âœ… Event-driven architecture
â”‚   â”‚   â””â”€â”€ container/          # âœ… Dependency injection
â”‚   â”œâ”€â”€ api/                    # âœ… API gateway (renamed from api-gateway)
â”‚   â”‚   â”œâ”€â”€ app/routers/        # âœ… Asset, forecast, collector endpoints
â”‚   â”‚   â”œâ”€â”€ app/middleware/     # âœ… Logging and CORS middleware
â”‚   â”‚   â””â”€â”€ main.py             # âœ… FastAPI application
â”‚   â”œâ”€â”€ dashboard/              # âœ… UNIFIED PLUGIN-BASED DASHBOARD  
â”‚   â”‚   â”œâ”€â”€ components/         # âœ… UI components
â”‚   â”‚   â”œâ”€â”€ core/               # âœ… Plugin manager and config
â”‚   â”‚   â”œâ”€â”€ plugins/            # âœ… Asset-specific plugins
â”‚   â”‚   â”œâ”€â”€ main.py             # âœ… Dashboard entry point
â”‚   â”‚   â””â”€â”€ config/             # âœ… Dashboard configuration
â”‚   â”œâ”€â”€ collectors/             # âœ… Data collection (migrated from src/dev)
â”‚   â”‚   â””â”€â”€ watch/              # âœ… Watch-specific scrapers
â”‚   â”œâ”€â”€ models/                 # âœ… ML pipeline (migrated from src/dev)
â”‚   â”‚   â””â”€â”€ forecasting/        # âœ… Forecasting models and data prep
â”‚   â”œâ”€â”€ utils/                  # âœ… Shared utilities (migrated from src/utils)
â”‚   â””â”€â”€ scripts/                # âœ… Entry point scripts
â”œâ”€â”€ data/                       # âœ… Data files (unchanged)
â”‚   â”œâ”€â”€ watches/                # âœ… Historical price data
â”‚   â”œâ”€â”€ images/                 # âœ… Watch images
â”‚   â””â”€â”€ output/                 # âœ… Model results
â””â”€â”€ tests/                      # âœ… Test files
```

#### 2. **Core Abstractions** âœ…
- **BaseAsset**: Abstract class for all tradeable assets
- **BaseDataCollector**: Abstract class for data collection
- **PricePoint**: Standardized price data schema
- **Forecast**: Standardized forecast output schema
- **EventBus**: Event-driven communication system
- **Container**: Dependency injection framework

#### 3. **API Gateway** âœ…
- **FastAPI-based**: RESTful API with OpenAPI documentation
- **Asset Management**: List, get, history endpoints
- **Forecasting**: Forecast generation and model management
- **Data Collection**: Collector status and trigger endpoints
- **Middleware**: Request logging and CORS handling

#### 4. **Plugin Architecture** âœ…
- **PluginManager**: Dynamic plugin loading and management
- **AssetPlugin**: Base class for asset-specific UI components
- **WatchPlugin**: Complete watch forecasting interface
- **Configuration**: JSON-based plugin configuration
- **Components**: Reusable sidebar, header, and navigation

### Technical Features Implemented

#### API Gateway Features
- **Health Check**: `/health` endpoint for system monitoring
- **Asset Endpoints**: `/api/v1/assets/{asset_type}`
- **Forecast Endpoints**: `/api/v1/forecasts/{asset_type}/{asset_id}`
- **Collector Endpoints**: `/api/v1/collectors/`
- **Error Handling**: Global exception handling
- **Documentation**: Auto-generated OpenAPI docs at `/docs`

#### Dashboard Features
- **Multi-Asset Support**: Plugin-based architecture for different asset types
- **Interactive UI**: Streamlit-based responsive interface
- **Real-time Updates**: Dynamic data fetching and visualization
- **Customization**: Theme and display configuration
- **Navigation**: Sidebar navigation with plugin detection

#### Watch Plugin Features
- **Market Overview**: Brand distribution and price trends
- **Technical Analysis**: Moving averages and volatility charts
- **Price Forecasting**: ML model selection and forecast visualization
- **Asset Selection**: Dynamic asset loading and filtering
- **Settings**: Asset-specific configuration options

### Development Experience Improvements

#### Code Organization
- **Modular Design**: Clear separation of concerns
- **Type Safety**: Full type hints throughout codebase
- **Error Handling**: Comprehensive exception handling
- **Logging**: Structured logging with appropriate levels
- **Documentation**: Inline documentation and examples

#### Developer Tools
- **Hot Reload**: Development server with auto-reload
- **API Documentation**: Interactive API docs and testing
- **Plugin Development**: Easy plugin creation and registration
- **Configuration**: JSON-based configuration management

### Testing the Implementation

#### Start API Gateway
```bash
cd src/api-gateway
pip install -r requirements.txt
python main.py
# Access API docs at http://localhost:8000/docs
```

#### Start Dashboard
```bash
cd src/dashboard
pip install -r requirements.txt
streamlit run main.py
# Access dashboard at http://localhost:8501
```

#### Test Plugin System
```bash
# Dashboard automatically loads WatchPlugin
# Navigate between Overview, Analysis, and Forecasting pages
# Test asset selection and filtering
```

### Next Steps: Phase 2 Preparation

#### Ready for Phase 2: Refactor Existing Code
1. **Move Watch Data**: Migrate `src/dev/scraper.py` to `data-collectors/watch-collector/`
2. **Refactor ML Pipeline**: Move `src/dev/modelling.py` to `ml-models/framework/`
3. **Migrate Dashboard**: Integrate existing `src/app/` components with new plugin system
4. **Update Data Structure**: Migrate `data/watches/` to new unified format

#### Integration Points
- **API Gateway** â†” **Existing Scrapers**: Connect via collector endpoints
- **Dashboard Plugins** â†” **Existing Analysis**: Integrate existing visualization logic
- **Core Library** â†” **Existing Models**: Standardize model interfaces
- **Event System** â†” **Workflow**: Add event-driven data pipeline

### Benefits Achieved

#### Scalability
- âœ… **Easy Asset Addition**: New assets require only new plugin implementation
- âœ… **Horizontal Scaling**: API gateway can scale independently
- âœ… **Modular Development**: Teams can work on different asset types independently

#### Maintainability
- âœ… **Clear Architecture**: Well-defined interfaces and abstractions
- âœ… **Separation of Concerns**: API, UI, and business logic separated
- âœ… **Plugin System**: Asset-specific code contained in plugins

#### Developer Experience
- âœ… **Type Safety**: Full type hints and validation
- âœ… **Documentation**: Auto-generated API docs and inline documentation
- âœ… **Hot Reload**: Fast development iteration
- âœ… **Standardization**: Consistent patterns across components

## Code Refactoring & Modernization âœ…

**COMPLETED** - Comprehensive code refactoring (June 2024)

### Refactoring Objectives
- Remove deprecated and unnecessary code
- Modernize dependencies and patterns
- Improve code quality and maintainability
- Consolidate redundant utilities

### Changes Implemented

#### 1. **Dependencies & Requirements** âœ…
- **Consolidated Requirements**: Merged `requirements.txt` and `requirements_scraping.txt`
- **Security-Focused Pinning**: Added upper bounds to prevent breaking changes
- **Modern Setup**: Created `setup.py` with optional dependencies for modular installation
- **Development Tools**: Added black, isort, mypy, pytest for code quality

```bash
# Install options
pip install -e .                    # Core functionality
pip install -e .[scraping]         # With web scraping
pip install -e .[api]              # With API gateway
pip install -e .[dev]              # With development tools
pip install -e .[all]              # Everything
```

#### 2. **Code Modernization** âœ…
- **Type Hints**: Added comprehensive type annotations throughout codebase
- **Modern Imports**: Replaced deprecated `sys.path` manipulation with proper relative imports
- **Path Handling**: Migrated from `os.path` to modern `pathlib.Path`
- **Warning Management**: Replaced global warning suppression with targeted context managers
- **Documentation**: Added module-level docstrings and improved function documentation

#### 3. **ML Pipeline Improvements** âœ…
- **Modern Metrics**: Added MAPE (Mean Absolute Percentage Error) for better evaluation
- **Data Classes**: Introduced `ModelMetrics` dataclass for structured metric handling
- **Plotting Updates**: Migrated to `seaborn-v0_8` style for modern visualizations
- **Configuration**: Centralized plot styling with `plt.rcParams.update()`

#### 4. **Scraping Component Updates** âœ…
- **Enhanced Error Handling**: Improved exception handling with structured logging
- **Modern Concurrency**: Updated `ThreadPoolExecutor` usage with `as_completed`
- **Type Safety**: Added type hints for all scraping functions
- **Documentation**: Comprehensive module and function documentation

#### 5. **Dashboard Modernization** âœ…
- **CSS Cleanup**: Removed unnecessary styles and modernized remaining CSS
- **Modern Patterns**: Updated to use current Streamlit best practices
- **Performance**: Optimized CSS with CSS variables and modern selectors
- **Responsive Design**: Improved mobile-friendly styling

#### 6. **File Organization** âœ…
- **Data Consolidation**: Merged `data/watch_data/` into `data/watches/`
- **Cleanup**: Removed redundant error screenshots and temporary files
- **GitIgnore**: Updated `.gitignore` with modern patterns for new platform structure
- **Structure**: Maintained clean directory hierarchy

#### 7. **Code Quality Improvements** âœ…

**Before Refactoring Issues:**
- Global warning suppression masking potential issues
- Deprecated `sys.path.insert()` usage
- Missing type hints reducing IDE support
- Redundant file structures
- Inconsistent error handling

**After Refactoring Benefits:**
- **Type Safety**: Full mypy compliance with proper type hints
- **Modern Imports**: Clean relative imports with fallback handling
- **Targeted Warnings**: Context-specific warning management
- **Clean Structure**: Consolidated data directories
- **Better Errors**: Structured logging with appropriate levels
- **Documentation**: Comprehensive docstrings for all modules

#### 8. **Development Experience** âœ…
- **Setup Script**: Modern `setup.py` with entry points and optional dependencies
- **Code Formatting**: Black, isort configuration for consistent style
- **Type Checking**: MyPy integration for static type analysis
- **Testing**: Pytest framework setup for comprehensive testing
- **Git Management**: Updated `.gitignore` for clean repository

### Migration Commands

#### Install Modern Dependencies
```bash
# Uninstall old packages if needed
pip uninstall -y seaborn matplotlib

# Install with new pinned versions
pip install -e .
```

#### Code Quality Checks
```bash
# Format code
black src/
isort src/

# Type checking
mypy src/

# Run tests
pytest tests/
```

#### Verify Modernization
```bash
# Check for deprecated patterns
grep -r "sys.path.insert" src/  # Should return no results
grep -r "warnings.filterwarnings.*ignore.*)" src/  # Should be minimal
```

### Backward Compatibility
- **API Unchanged**: All public interfaces remain the same
- **Data Format**: Existing CSV files and models continue to work
- **Configuration**: Existing settings and configurations preserved
- **Functionality**: No breaking changes to core features

### Performance Improvements
- **Import Speed**: Faster module loading with proper imports
- **Memory Usage**: Reduced memory footprint from cleaned dependencies
- **Error Handling**: Better error messages and debugging information
- **Code Quality**: Enhanced IDE support with type hints

## Directory Restructuring & Consolidation âœ…

**COMPLETED** - Unified project structure (June 2024)

### Problem Solved
The project had **redundant directory structures** with both `src/` and `src/` containing overlapping functionality. This created confusion and maintenance overhead.

### Solution Implemented
**Consolidated everything into a single `src/` directory** with clear separation of concerns:

#### Migration Completed
- âœ… **`src/app/` â†’ `src/dashboard/legacy/`** (existing Streamlit app)
- âœ… **`src/dev/` â†’ `src/collectors/watch/` & `src/models/forecasting/`** (scrapers & ML)
- âœ… **`src/utils/` â†’ `src/utils/`** (shared utilities)
- âœ… **Root scripts â†’ `src/scripts/`** (entry points)
- âœ… **Updated all imports and path references**
- âœ… **Removed redundant `src/` directory**

#### New Entry Points
```bash
# Legacy Streamlit dashboard
python src/scripts/run_app.py

# Modern plugin-based dashboard  
python src/scripts/run_modern_dashboard.py

# API gateway
python src/scripts/run_api.py

# Or using setup.py entry points:
pip install -e .
watch-forecaster              # Legacy dashboard
watch-forecaster-modern       # Modern dashboard  
watch-api                     # API gateway
```

#### Benefits Achieved
- **Single Source of Truth**: All code in one unified directory structure
- **Clear Organization**: Logical separation by function (collectors, models, dashboards, api)
- **Dual Dashboard Support**: Both legacy and modern dashboards coexist
- **Simplified Imports**: Clean relative imports throughout
- **Better Navigation**: Intuitive directory names and structure
- **Future Ready**: Structure supports easy addition of new asset types

#### Backward Compatibility
- âœ… **All existing functionality preserved**
- âœ… **Data files and configurations unchanged**
- âœ… **Legacy dashboard fully functional**
- âœ… **API contracts maintained**

### Directory Structure Rationale

#### `core/` - Shared Foundation
Contains abstract base classes, schemas, and cross-cutting concerns used by all components.

#### `api/` - External Interface  
FastAPI-based REST API providing programmatic access to all platform capabilities.

#### `dashboard/` - User Interfaces
- **`legacy/`**: Battle-tested Streamlit app for immediate use
- **`modern/`**: Plugin-based architecture for scalability

#### `collectors/` - Data Ingestion
Asset-specific data collection modules organized by asset type.

#### `models/` - Intelligence Layer
Machine learning models and data processing pipelines.

#### `utils/` - Supporting Infrastructure
Shared utilities like Selenium drivers, helper functions, etc.

#### `scripts/` - Entry Points
Clean entry point scripts for different application modes.

## Repository Information
- **Remote Origin**: https://github.com/simplysindy/ACTP.git
- **Main Branch**: main
- **Commit regularly and push frequently for backup and collaboration**